{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "268469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.converter import pdf_converter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9748cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_list = pdf_converter(\"documents/\", min_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9115b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "docs = array_list\n",
    "doc_emb = model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67d4089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 384), 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape,len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb49f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Load the model\n",
    "\n",
    "\n",
    "#Encode query and documents\n",
    "\n",
    "\n",
    "def compute_score(doc_emb, query_emb, top_k=3):\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "    return [docs[i] for i in np.argsort(scores)[-5:]]\n",
    "    #Combine docs & scores\n",
    "#     doc_score_pairs = list(zip(docs, scores))\n",
    "    #Sort by decreasing score\n",
    "#     doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "#     return doc_score_pairs\n",
    "#     for doc, score in doc_score_pairs[0:5]:\n",
    "#         print(score, doc)\n",
    "#     return [doc for doc,score in doc_score_pairs[:top_k]]\n",
    "#     return doc_score_pairs\n",
    "#Output passages & scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d13dd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name =\"deepset/roberta-base-squad2-distilled\" #\n",
    "# model_name=\"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "# a) Get predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1420a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qa_model(query, relevent_docs):\n",
    "    QA_input = [{\n",
    "        'question': query,#'Why is model conversion important?',\n",
    "        'context': doc#'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "    } for doc in relevent_docs]\n",
    "    res = nlp(QA_input)\n",
    "    return res\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def get_answer(query):\n",
    "    query_emb = model.encode(query)\n",
    "    relevent_docs = compute_score(doc_emb, query_emb,top_k=5)\n",
    "    answer = qa_model(query, relevent_docs)\n",
    "    __  = {key.update({\"paragraph\":relevent_docs.pop(0), \"score\":round(key[\"score\"], 3)}) for key in answer }\n",
    "    answer_df = pd.DataFrame(answer)\n",
    "    return answer_df.sort_values([\"score\"],ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08aa0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in doc_score_pairs[0:5]:\n",
    "#     print(doc)\n",
    "temp = get_answer(\"where is he born\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98d4d419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505</td>\n",
       "      <td>48</td>\n",
       "      <td>57</td>\n",
       "      <td>Elanthoor</td>\n",
       "      <td>Mohanlal Viswanathan was born in the village o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.412</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>Mohanlal Birth Place</td>\n",
       "      <td>24. Mohanlal Birth Place  Archived  25 April 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>Mohanlal Viswanathan</td>\n",
       "      <td>Born Mohanlal Viswanathan[1][2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>Ujala Ali</td>\n",
       "      <td>17. Khan, Ujala Ali (14 September 2013). \"Reig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>.</td>\n",
       "      <td>Relatives K. Balaji (father-in-law)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  start  end                answer  \\\n",
       "0  0.505     48   57             Elanthoor   \n",
       "3  0.412      4   24  Mohanlal Birth Place   \n",
       "4  0.043      5   25  Mohanlal Viswanathan   \n",
       "1  0.006     10   19             Ujala Ali   \n",
       "2  0.000     11   12                     .   \n",
       "\n",
       "                                           paragraph  \n",
       "0  Mohanlal Viswanathan was born in the village o...  \n",
       "3  24. Mohanlal Birth Place  Archived  25 April 2...  \n",
       "4                    Born Mohanlal Viswanathan[1][2]  \n",
       "1  17. Khan, Ujala Ali (14 September 2013). \"Reig...  \n",
       "2                Relatives K. Balaji (father-in-law)  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a03cf997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"which district was  he born\"\n",
    "# query_emb = model.encode(query)\n",
    "\n",
    "# relevent_docs = compute_score(doc_emb, query_emb,top_k=5)\n",
    "# relevent_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7868b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"\"\"No matter how meticulous the plan, disruptors are more than likely to appear at some \\\n",
    "point during large-scale transformations. Careful planning is no substitute for flexibility, \\\n",
    "so the program management approach needs in-built resilience and adaptability. When \\\n",
    "a hurdle appears, the teams need to adjust quickly, refocus, and be open to adopting \\\n",
    "an incremental micro delivery approach when an initial macro delivery was originally \\ \n",
    "planned, for example. With the right approach and mindset, disruptions don’t necessarily \\\n",
    "have to result in extending key delivery timelines.No matter how meticulous the plan, disruptors are more than likely to appear at some \\\n",
    "point during large-scale transformations. Careful planning is no substitute for flexibility, \\\n",
    "so the program management approach needs in-built resilience and adaptability. When \\\n",
    "a hurdle appears, the teams need to adjust quickly, refocus, and be open to adopting \\\n",
    "an incremental micro delivery approach when an initial macro delivery was originally \\ \n",
    "planned, for example. With the right approach and mindset, disruptions don’t necessarily \\\n",
    "have to result in extending key delivery timelines. No matter how meticulous the plan, disruptors are more than likely to appear at some \\\n",
    "point during large-scale transformations. Careful planning is no substitute for flexibility, \\\n",
    "so the program management approach needs in-built resilience and adaptability. When \\\n",
    "a hurdle appears, the teams need to adjust quickly, refocus, and be open to adopting \\\n",
    "an incremental micro delivery approach when an initial macro delivery was originally \\ \n",
    "planned, for example. With the right approach and mindset, disruptions don’t necessarily \\\n",
    "have to result in extending key delivery timelines.No matter how meticulous the plan, disruptors are more than likely to appear at some \\\n",
    "point during large-scale transformations. Careful planning is no substitute for flexibility, \\\n",
    "so the program management approach needs in-built resilience and adaptability. When \\\n",
    "a hurdle appears, the teams need to adjust quickly, refocus, and be open to adopting \\\n",
    "an incremental micro delivery approach when an initial macro delivery was originally \\ \n",
    "planned, for example. With the right approach and mindset, disruptions don’t necessarily \\\n",
    "have to result in extending key delivery timelines.\"\"\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# print(\"Sentence embeddings:\")\n",
    "# print(sentence_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cf27e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([921])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef62fd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "# sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=False, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# print(\"Sentence embeddings:\")\n",
    "# print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce310127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 921])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
